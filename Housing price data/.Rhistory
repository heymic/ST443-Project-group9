df[(df$PoolArea > 0) & is.na(df$PoolQC),c('PoolQC','PoolArea')]
df[2421,'PoolQC'] = 'Ex'
df[2504,'PoolQC'] = 'Ex'
df[2600,'PoolQC'] = 'Fa'
df$PoolQC[is.na(df$PoolQC)] = 'None'
garage.cols <- c('GarageArea', 'GarageCars', 'GarageQual', 'GarageFinish', 'GarageCond', 'GarageType')
#df[is.na(df$GarageCond),garage.cols]
#length(which(df$GarageYrBlt == df$YearBuilt))
df[(df$GarageArea > 0) & is.na(df$GarageYrBlt), c(garage.cols, 'GarageYrBlt')]
df$GarageYrBlt[2127] <- df$YearBuilt[2127]
df[2127, 'GarageQual'] <- Mode(df$GarageQual)
df[2127, 'GarageFinish'] <- Mode(df$GarageFinish)
df[2127, 'GarageCond'] <- Mode(df$GarageCond)
df$GarageYrBlt[which(is.na(df$GarageYrBlt))] <- 0
for(i in garage.cols){
if (sapply(df[i], is.numeric) == TRUE){
df[,i][which(is.na(df[,i]))] <- 0
}
else{
df[,i][which(is.na(df[,i]))] <- "None"
}
}
df$KitchenQual[which(is.na(df$KitchenQual))] <- Mode(df$KitchenQual)
df[is.na(df$MSZoning),c('MSZoning','MSSubClass')]
table(df$MSZoning, df$MSSubClass)
df$MSZoning[c(2217, 2905)] = 'RL'
df$MSZoning[c(1916, 2251)] = 'RM'
df$LotFrontage[which(is.na(df$LotFrontage))] <- median(df$LotFrontage,na.rm = T)
df$Alley[which(is.na(df$Alley))] <- "None"
#df[(df$MasVnrArea > 0) & (is.na(df$MasVnrType)),c('MasVnrArea','MasVnrType')]
df[2611, 'MasVnrType'] = 'BrkFace'
df$MasVnrType[is.na(df$MasVnrType)] = 'None'
df$MasVnrArea[is.na(df$MasVnrArea)] = 0
for(i in colnames(df[,sapply(df, is.character)])){
if (sum(is.na(df[,i])) < 5){
df[,i][which(is.na(df[,i]))] <- Mode(df[,i])
}
}
for(i in colnames(df[,sapply(df, is.integer)])){
if (sum(is.na(df[,i])) < 5){
df[,i][which(is.na(df[,i]))] <- median(df[,i], na.rm = T)
}
}
for(i in colnames(df[,sapply(df, is.character)])){
df[,i][which(is.na(df[,i]))] <- "None"
}
colSums(sapply(df, is.na))
sum(is.na(df)) == 1459
train_df <- df[df$isTrain==1,]
test_df <- df[df$isTrain==0,]
train_df$isTrain <- NULL
test_df$isTrain <- NULL
test_df$SalePrice <- NULL
train_df$MSSubClass <- as.factor(train_df$MSSubClass)
test_df$MSSubClass <- as.factor(test_df$MSSubClass)
train_df$OverallQual <- as.factor(train_df$OverallQual)
test_df$OverallQual <- as.factor(test_df$OverallQual)
train_df$OverallCond <- as.factor(train_df$OverallCond)
test_df$OverallCond <- as.factor(test_df$OverallCond)
for(i in colnames(train_df[,sapply(train_df, is.character)])){
train_df[,i] <- as.factor(train_df[,i])
}
for(i in colnames(test_df[,sapply(test_df, is.character)])){
test_df[,i] <- as.factor(test_df[,i])
}
#Check is some there are more levels in some of the categorical factors in the testing compared to the training
for(i in colnames(train_df[,sapply(train_df, is.factor)])){
if (length(levels(train_df[,i])) < length(levels(test_df[,i]))) {
print(i)
print(levels(train_df[,i]))
print(levels(test_df[,i]))
}
}
#150 is appears in the testing data, but doesn't appear in the training data
df[df$MSSubClass == 150,]
df[df$MSSubClass == 150,"MSSubClass"] <- 120
hist(df$SalePrice)
df$SalePrice <- log(df$SalePrice)
for(i in colnames(df[,sapply(df, is.character)])){
df[,i] <- as.factor(df[,i])
}
df$MSSubClass <- as.factor(df$MSSubClass)
df$OverallQual <- as.factor(df$OverallQual)
df$OverallCond <- as.factor(df$OverallCond)
### THINGS TO CONSIDER:
#df$GarageYrBlt <- as.factor(df$GarageYrBlt) # treat as factor as some of them are '0'
#add years as dummies - POSSIBILITY - but a problem appears, the algorithms cannot treat categorical varibles with more than 55levels
#df$YearBuilt <- as.factor(df$YearBuilt)
#df$YearRemodAdd <- as.factor(df$YearRemodAdd)
#df$YrSold <- as.factor(df$YrSold)
train_df <- df[df$isTrain==1,]
test_df <- df[df$isTrain==0,]
train_df$isTrain <- NULL
test_df$isTrain <- NULL
test_df$SalePrice <- NULL
str(df)
str(test_df)
str(df)
X_train<- model.matrix(SalePrice~.-1, data = train_df)
y_train <- train_df$SalePrice
X_test <- model.matrix(~.-1, data=test_df)
lm_fit_all = lm(SalePrice ~., data = train_df)
#summary(lm_fit_all)
X_train<- model.matrix(SalePrice~.-1, data = train_df)
y_train <- train_df$SalePrice
X_test <- model.matrix(~.-1, data=test_df)
lm_fit_all = lm(SalePrice ~., data = train_df)
#summary(lm_fit_all)
prediction_LR_ALL <- predict(lm_fit_all, test_df, type="response")
prediction_LR_ALL <- as.data.frame(prediction_LR_ALL)
prediction_LR_ALL <- cbind(Id = rownames(test_df), SalesPrice = prediction_LR_ALL)
View(prediction_LR_ALL)
prediction_LR_ALL_log <- as.data.frame(prediction_LR_ALL)
prediction_LR_ALL <- exp(prediction_LR_ALL_log)
prediction_LR_ALL_log <- as.data.frame(prediction_LR_ALL)
prediction_LR_ALL_log <- predict(lm_fit_all, test_df, type="response")
prediction_LR_ALL <- exp(prediction_LR_ALL_log)
prediction_LR_ALL <- cbind(Id = rownames(test_df), SalesPrice = prediction_LR_ALL)
write.table(prediction_LR_ALL, file="prediction_LR_ALL.csv",col.names = c("Id","SalePrice"), sep =',', row.names = FALSE)
cv.lasso <-cv.glmnet(X_train, y_train, nfolds = 10, alpha = 1)
plot(cv.lasso)
#here is an example why should we apply the one standard deviation error rule!
penalty_min <- cv.lasso$lambda.min #optimal lambda
penalty_1se <- cv.lasso$lambda.1se # 1 Standard Error Apart
fit.lasso_min <-glmnet(X_train, y_train, alpha = 1, lambda = penalty_min) #estimate the model with min lambda
fit.lasso_1se <-glmnet(X_train, y_train, alpha = 1, lambda = penalty_1se) #estimate the model with 1se apart
prediction_LASSO_min_log <- predict(fit.lasso_min, X_test)
prediction_LASSO_1se_log <- predict(fit.lasso_1se, X_test)
prediction_LASSO_min <- exp(prediction_LASSO_min_log)
prediction_LASSO_1se <- exp(prediction_LASSO_1se_log)
prediction_LASSO_min <-cbind(Id = rownames(test_df), SalesPrice = prediction_LASSO_min)
prediction_LASSO_1se <- cbind(Id = rownames(test_df), SalesPrice = prediction_LASSO_1se)
write.table(prediction_LASSO_min, file="prediction_LASSO_min.csv",col.names = c("Id", "SalePrice"), sep =',', row.names = FALSE)
write.table(prediction_LASSO_1se, file="prediction_LASSO_1se.csv",col.names = c("Id", "SalePrice"), sep =',', row.names = FALSE)
## Regression Tree
tree.SalePrice <-tree(SalePrice~., data = train_df)
library(boot)
library(leaps)
library(tree) # Normal Tree
library(randomForest) # random Forest
library(xgboost) # boosting trees
library(Matrix) #?
library(methods) #?
library(caret) # for tuning xgboost
## Regression Tree
tree.SalePrice <-tree(SalePrice~., data = train_df)
summary(tree.SalePrice)
plot(tree.SalePrice)
text(tree.SalePrice, pretty=1)
cv.SalePrice <-cv.tree(tree.SalePrice, K = 10)
plot(cv.SalePrice$size, cv.SalePrice$dev, type="b")
## In this case, the most complex tree is selected by cross-validation
## However, if we wish to prune the tree, we could do so as follows using prune.tree() function
prune.SalePrice <-prune.tree(tree.SalePrice, best=11)
plot(prune.SalePrice)
text(prune.SalePrice, pretty=1)
cv.SalePrice$dev # no PRUNING DONE
cv.SalePrice <-cv.tree(tree.SalePrice, K = 100)
plot(cv.SalePrice$size, cv.SalePrice$dev, type="b")
## In this case, the most complex tree is selected by cross-validation
## However, if we wish to prune the tree, we could do so as follows using prune.tree() function
prune.SalePrice <-prune.tree(tree.SalePrice, best=11)
plot(prune.SalePrice)
text(prune.SalePrice, pretty=1)
cv.SalePrice$dev # no PRUNING DONE
?tree
## Regression Tree
tree.SalePrice <-tree(SalePrice~., data = train_df)
summary(tree.SalePrice)
plot(tree.SalePrice)
text(tree.SalePrice, pretty=1)
cv.SalePrice <-cv.tree(tree.SalePrice, K = 10)
plot(cv.SalePrice$size, cv.SalePrice$dev, type="b")
## In this case, the most complex tree is selected by cross-validation
## However, if we wish to prune the tree, we could do so as follows using prune.tree() function
prune.SalePrice <-prune.tree(tree.SalePrice, best=11)
plot(prune.SalePrice)
text(prune.SalePrice, pretty=1)
cv.SalePrice$dev # no PRUNING DONE
cv.SalePrice <-cv.tree(tree.SalePrice, K = 10)
plot(cv.SalePrice$size, cv.SalePrice$dev, type="b")
## In this case, the most complex tree is selected by cross-validation
## However, if we wish to prune the tree, we could do so as follows using prune.tree() function
prune.SalePrice <-prune.tree(tree.SalePrice, best=100)
plot(prune.SalePrice)
text(prune.SalePrice, pretty=1)
cv.SalePrice$dev # no PRUNING DONE
prediction_TREE_log <- predict(prune.SalePrice ,test_df)
prediction_TREE <- exp(prediction_TREE_log)
prediction_TREE <- cbind(Id = rownames(X_test), SalesPrice = prediction_TREE)
write.table(prediction_TREE, file="prediction_TREE.csv",col.names = c("Id", "SalePrice"), sep =',', row.names = FALSE)
##Bagging
library(randomForest)
set.seed(1)
## Recall that bagging is simply a special case of a random forest with m=p, here we use mtry=13, i.e. bagging would be done
bag.SalePrice <-randomForest(SalePrice~., data=train_df, mtry= 79, importance=TRUE)
bag.SalePrice
prediction_bag <- predict(bag.SalePrice, newdata = test_df)
prediction_bag_log <- predict(bag.SalePrice, newdata = test_df)
prediction_bag <- exp(prediction_bag_log)
prediction_bag <- cbind(Id = rownames(X_test), SalesPrice = prediction_bag)
write.table(prediction_bag, file="prediction_bag.csv",col.names = c("Id", "SalePrice"), sep =',', row.names = FALSE)
## We could change the number of trees grown by randomForest() using ntree argument
RF_p3.SalePrice <-randomForest(SalePrice~., data=train_df, mtry = 26, importance=TRUE)
prediction_RF_p3_log <- predict(RF_p3.SalePrice, newdata = test_df)
prediction_RF_p3 <- exp(prediction_RF_p3_log)
prediction_RF_p3 <- cbind(Id = names(X_test), SalesPrice = prediction_RF_p3)
write.table(prediction_RF_p3, file="prediction_RF_p3.csv",col.names = c("Id", "SalePrice"), sep =',', row.names = FALSE)
prediction_RF_p3_log <- predict(RF_p3.SalePrice, newdata = test_df)
prediction_RF_p3 <- exp(prediction_RF_p3_log)
prediction_RF_p3 <- cbind(Id = names(X_test), SalesPrice = prediction_RF_p3)
write.table(prediction_RF_p3, file="prediction_RF_p3.csv",col.names = c("Id", "SalePrice"), sep =',', row.names = FALSE)
prediction_RF_p3_log <- predict(RF_p3.SalePrice, newdata = test_df)
prediction_RF_p3 <- exp(prediction_RF_p3_log)
prediction_RF_p3 <- cbind(Id = rownames(X_test), SalesPrice = prediction_RF_p3)
write.table(prediction_RF_p3, file="prediction_RF_p3.csv",col.names = c("Id", "SalePrice"), sep =',', row.names = FALSE)
## We could change the number of trees grown by randomForest() using ntree argument
RF.SalePrice <-randomForest(SalePrice~., data=train_df, importance=TRUE)
prediction_RF <- predict(RF.SalePrice, newdata = test_df)
prediction_RF <- cbind(Id = rownames(X_test), SalesPrice = prediction_RF)
write.table(prediction_RF, file="prediction_RF.csv",col.names = c("Id", "SalePrice"), sep =',', row.names = FALSE)
prediction_RF_log <- predict(RF.SalePrice, newdata = test_df)
prediction_RF <- exp(prediction_RF_log)
prediction_RF <- cbind(Id = rownames(X_test), SalesPrice = prediction_RF)
write.table(prediction_RF, file="prediction_RF.csv",col.names = c("Id", "SalePrice"), sep =',', row.names = FALSE)
x_train_df <- train_df
x_train_df$SalePrice <- NULL
results <- rfcv(x_train_df, y_train, cv.fold=10, scale="log", step=0.7)
resuls
t
results$error.cv
which.min(results$error.cv)
results$error.cv
min(results$error.cv)
print(which.min(results$error.cv))
print(min(results$error.cv))
which.min(results$error.cv)
results$error.cv
## We could change the number of trees grown by randomForest() using ntree argument
RF.SalePrice_tuned <-randomForest(SalePrice~., data=train_df, importance=TRUE, ntree = 1000, mtry=55)
prediction_RF_tuned_log <- predict(RF.SalePrice_tuned, newdata = test_df)
prediction_RF_tuned <- exp(prediction_RF_tuned_log)
prediction_RF_tuned_log <- predict(RF.SalePrice_tuned, newdata = test_df)
prediction_RF_tuned <- exp(prediction_RF_tuned_log)
prediction_RF_tuned <- cbind(Id = rownames(X_test), SalesPrice = prediction_RF_tuned)
write.table(prediction_RF_tuned, file="prediction_RF_tuned.csv",col.names = c("Id", "SalePrice"), sep =',', row.names = FALSE)
library(gbm)
library(gbm)
set.seed(1)
boost.train = gbm(SalePrice ~. , data=train_df,
distribution = "gaussian",
n.trees = 1000,
shrinkage = 0.05,
interaction.depth = 5,
bag.fraction = 0.66,
cv.folds = 10,
verbose = FALSE,
n.cores = 8)
min(boost.train$cv.error)
#attributes(boost.train)
bestTreeForPrediction = gbm.perf(boost.train)
prediction_BOOST_log <- predict(boost.train, test_df)
prediction_BOOST <- exp(prediction_BOOST_log)
prediction_BOOST <- cbind(Id = rownames(X_test), SalesPrice = prediction_BOOST)
write.table(prediction_BOOST, file="prediction_BOOST.csv",col.names = c("Id", "SalePrice"), sep =',', row.names = FALSE)
#setup
library(parallel)
detectCores() #=> 8
dtrain <- xgb.DMatrix(X_train, label = y_train)
best_param2 = list()
best_rmse = Inf
best_rmse_index = 0
best_seednumber = 1234
for (iter in 1:40) {
param <- list(objective = "reg:linear",
max_depth = sample(1:10, 1),
eta = runif(1, .01, .05),
gamma = runif(1, 0.0, 0.013),
subsample = runif(1, .7, .8),
colsample_bytree = runif(1, .6, .7),
min_child_weight = sample(1:10, 1)
)
cv.nround = 1000
cv.nfold = 10
seed.number = sample.int(10000, 1)[[1]]
set.seed(seed.number)
mdcv <- xgb.cv(data=dtrain, params = param, nthread=8,
nfold=cv.nfold, nrounds=cv.nround,
verbose = F, early.stop.rounds=8, maximize=FALSE)
min_rmse = min(mdcv$evaluation_log[,test_rmse_mean])
min_rmse_index = which.min(mdcv$evaluation_log[,test_rmse_mean])
if (min_rmse < best_rmse) {
best_rmse = min_rmse
best_rmse_index = min_rmse_index
best_seednumber = seed.number
best_param2 = param
}
print(iter)
}
print(best_param2) # rmse = 0.12434
#print(best_param) #rmse.cv.error = 0.12567
nround = best_rmse_index
set.seed(best_seednumber)
model_XGB_tune2 <- xgb.train(data=dtrain, params=best_param2, nrounds=nround, nthread=8)
prediction_XGB_tune2_log <- predict(model_XGB_tune2, X_test)
prediction_XGB_tune2 <- exp(prediction_XGB_tune2_log)
prediction_XGB_tune2 <- cbind(Id = rownames(X_test), SalesPrice = prediction_XGB_tune2)
write.table(prediction_XGB_tune2, file="prediction_XBG_tune2.csv",col.names = c("Id", "SalePrice"), sep =',', row.names = FALSE)
best_param2 = list()
best_rmse = Inf
best_rmse_index = 0
best_seednumber = 1234
for (iter in 1:20) {
param <- list(objective = "reg:linear",
max_depth = sample(2:6, 1),
eta = runif(1, .01, .05),
gamma = runif(1, 0.0, 0.013),
subsample = runif(1, .7, .8),
colsample_bytree = runif(1, .6, .7),
min_child_weight = sample(1:10, 1)
)
cv.nround = 1000
cv.nfold = 10
seed.number = sample.int(10000, 1)[[1]]
set.seed(seed.number)
mdcv <- xgb.cv(data=dtrain, params = param, nthread=8,
nfold=cv.nfold, nrounds=cv.nround,
verbose = F, early.stop.rounds=8, maximize=FALSE)
min_rmse = min(mdcv$evaluation_log[,test_rmse_mean])
min_rmse_index = which.min(mdcv$evaluation_log[,test_rmse_mean])
if (min_rmse < best_rmse) {
best_rmse = min_rmse
best_rmse_index = min_rmse_index
best_seednumber = seed.number
best_param2 = param
}
print(iter)
}
print(best_param2) # rmse = 0.12608
#print(best_param) #rmse.cv.error = 0.12567
nround = best_rmse_index
set.seed(best_seednumber)
model_XGB_tune2 <- xgb.train(data=dtrain, params=best_param2, nrounds=nround, nthread=8)
nround = best_rmse_index
set.seed(best_seednumber)
model_XGB_tune2 <- xgb.train(data=dtrain, params=best_param2, nrounds=nround, nthread=8)
prediction_XGB_tune2_log <- predict(model_XGB_tune2, X_test)
prediction_XGB_tune2 <- exp(prediction_XGB_tune2_log)
prediction_XGB_tune2 <- cbind(Id = rownames(X_test), SalesPrice = prediction_XGB_tune2)
write.table(prediction_XGB_tune2, file="prediction_XBG_tune2.csv",col.names = c("Id", "SalePrice"), sep =',', row.names = FALSE)
#colSums(sapply(df, is.na))
sum(is.na(df)) == 1459
print(best_param2) # rmse = 0.12608
#print(best_param) #rmse.cv.error = 0.12567
nround = best_rmse_index
set.seed(best_seednumber)
model_XGB_tune2 <- xgb.train(data=dtrain, params=best_param2, nrounds=nround, nthread=8)
prediction_XGB_tune2_log <- predict(model_XGB_tune2, X_test)
prediction_XGB_tune2 <- exp(prediction_XGB_tune2_log)
prediction_XGB_tune2 <- cbind(Id = rownames(X_test), SalesPrice = prediction_XGB_tune2)
write.table(prediction_XGB_tune2, file="prediction_XBG_tune2.csv",col.names = c("Id", "SalePrice"), sep =',', row.names = FALSE)
set.seed(1)
boost.train = gbm(SalePrice ~. , data=train_df,
distribution = "gaussian",
n.trees = 1000,
shrinkage = 0.05,
interaction.depth = 5,
bag.fraction = 0.66,
cv.folds = 10,
verbose = FALSE,
n.cores = 8)
min(boost.train$cv.error)
set.seed(1)
boost.train = gbm(SalePrice ~. , data=train_df,
distribution = "gaussian",
n.trees = 1000,
shrinkage = 0.05,
interaction.depth = 5,
bag.fraction = 1,
cv.folds = 10,
verbose = FALSE,
n.cores = 8)
min(boost.train$cv.error)
set.seed(1)
boost.train = gbm(SalePrice ~. , data=train_df,
distribution = "gaussian",
n.trees = 1000,
shrinkage = 0.05,
interaction.depth = 5,
bag.fraction = 0.4,
cv.folds = 10,
verbose = FALSE,
n.cores = 8)
min(boost.train$cv.error)
set.seed(1)
boost.train = gbm(SalePrice ~. , data=train_df,
distribution = "gaussian",
n.trees = 1000,
shrinkage = 0.02,
interaction.depth = 5,
bag.fraction = 0.66,
cv.folds = 10,
verbose = FALSE,
n.cores = 8)
min(boost.train$cv.error)
set.seed(1)
boost.train = gbm(SalePrice ~. , data=train_df,
distribution = "gaussian",
n.trees = 1000,
shrinkage = 0.01,
interaction.depth = 5,
bag.fraction = 0.66,
cv.folds = 10,
verbose = FALSE,
n.cores = 8)
min(boost.train$cv.error)
set.seed(1)
boost.train = gbm(SalePrice ~. , data=train_df,
distribution = "gaussian",
n.trees = 1000,
shrinkage = 0.05,
interaction.depth = 2,
bag.fraction = 0.66,
cv.folds = 10,
verbose = FALSE,
n.cores = 8)
min(boost.train$cv.error)
set.seed(1)
boost.train = gbm(SalePrice ~. , data=train_df,
distribution = "gaussian",
n.trees = 1000,
shrinkage = 0.05,
interaction.depth = 1,
bag.fraction = 0.66,
cv.folds = 10,
verbose = FALSE,
n.cores = 8)
min(boost.train$cv.error)
set.seed(1)
boost.train = gbm(SalePrice ~. , data=train_df,
distribution = "gaussian",
n.trees = 1000,
shrinkage = 0.05,
interaction.depth = 2,
bag.fraction = 0.66,
cv.folds = 10,
verbose = FALSE,
n.cores = 8)
min(boost.train$cv.error)
set.seed(1)
boost.train = gbm(SalePrice ~. , data= X_train,
distribution = "gaussian",
n.trees = 1000,
shrinkage = 0.05,
interaction.depth = 2,
bag.fraction = 0.66,
cv.folds = 10,
verbose = FALSE,
n.cores = 8)
gbm
?gbm
set.seed(1)
boost.train = gbm.fit(x = X_train, y = y_train,
distribution = "gaussian",
n.trees = 1000,
shrinkage = 0.05,
interaction.depth = 2,
bag.fraction = 0.66,
cv.folds = 10,
verbose = FALSE,
n.cores = 8)
set.seed(1)
boost.train = gbm.fit( X_train, y_train,
distribution = "gaussian",
n.trees = 1000,
shrinkage = 0.05,
interaction.depth = 2,
bag.fraction = 0.66,
cv.folds = 10,
verbose = FALSE,
n.cores = 8)
set.seed(1)
boost.train = gbm(SalePrice ~. , data= X_train,
distribution = "gaussian",
n.trees = 1000,
shrinkage = 0.05,
interaction.depth = 2,
bag.fraction = 0.66,
cv.folds = 10,
verbose = FALSE,
n.cores = 8)
set.seed(1)
boost.train = gbm(SalePrice ~. , data=train_df,
distribution = "gaussian",
n.trees = 1000,
shrinkage = 0.05,
interaction.depth = 2,
bag.fraction = 0.66,
cv.folds = 10,
verbose = FALSE,
n.cores = 8)
min(boost.train$cv.error)
results <- rfcv(x_train_df, y_train, cv.fold=10, scale="log", step=0.9)
results$error.cv
?rfcv
?randomForest
## We could change the number of trees grown by randomForest() using ntree argument
RF.SalePrice_tuned <-randomForest(SalePrice~., data=train_df, importance=TRUE, ntree = 1000, mtry=22)
prediction_RF_tuned_log <- predict(RF.SalePrice_tuned, newdata = test_df)
prediction_RF_tuned <- exp(prediction_RF_tuned_log)
prediction_RF_tuned <- cbind(Id = rownames(X_test), SalesPrice = prediction_RF_tuned)
write.table(prediction_RF_tuned, file="prediction_RF_tuned.csv",col.names = c("Id", "SalePrice"), sep =',', row.names = FALSE)
