---
title: "LSE ST443, Part 1 Regression"
author: "GROUP 9"
date: "Michealmas Term 2017"
output: pdf_document
---
```{r, warning=FALSE, message=FALSE}
library(tree) # Normal Tree
library(glmnet) # Lasso/Ridge
library(randomForest) # random Forest
library(xgboost) # boosting trees
library(caret) # for tuning xgboost
library(gbm) # for the gradient boosting model
```

Generating matrix from the data frames, no intercept

```{r, warning=FALSE, message=FALSE}
X_train<- model.matrix(SalePrice~.-1, data = train_df)
y_train <- train_df$SalePrice
X_test <- model.matrix(~.-1, data=test_df)
```

### Linear Regression

Regression with all the parameters - just as a benchmark

```{r, eval = FALSE}
lm_fit_all = lm(SalePrice ~., data = train_df) 
#summary(lm_fit_all)
```

```{r,   eval = FALSE}
prediction_LR_ALL_log <- predict(lm_fit_all, test_df, type="response")
prediction_LR_ALL <- exp(prediction_LR_ALL_log)
```

```{r,  eval = FALSE}
prediction_LR_ALL <- cbind(Id = rownames(test_df), SalesPrice = prediction_LR_ALL)
```

```{r,  eval = FALSE}
write.table(prediction_LR_ALL, file="prediction_LR_ALL.csv",col.names = c("Id","SalePrice"), sep =',', row.names = FALSE)
```


### Lasso

```{r}
cv.lasso <-cv.glmnet(X_train, y_train, nfolds = 10, alpha = 1)
plot(cv.lasso)
```

```{r,  eval = FALSE}
penalty_min <- cv.lasso$lambda.min #optimal lambda
penalty_1se <- cv.lasso$lambda.1se # 1 Standard Error Apart
fit.lasso_min <-glmnet(X_train, y_train, alpha = 1, lambda = penalty_min) #estimate the model with min lambda
fit.lasso_1se <-glmnet(X_train, y_train, alpha = 1, lambda = penalty_1se) #estimate the model with 1se apart
```

```{r,  eval = FALSE}
prediction_LASSO_min_log <- predict(fit.lasso_min, X_test)
prediction_LASSO_1se_log <- predict(fit.lasso_1se, X_test)

prediction_LASSO_min <- exp(prediction_LASSO_min_log)
prediction_LASSO_1se <- exp(prediction_LASSO_1se_log)
```

```{r, eval = FALSE}
prediction_LASSO_min <-cbind(Id = rownames(test_df), SalesPrice = prediction_LASSO_min)
prediction_LASSO_1se <- cbind(Id = rownames(test_df), SalesPrice = prediction_LASSO_1se)
```

```{r, eval = FALSE}
write.table(prediction_LASSO_min, file="prediction_LASSO_min.csv",col.names = c("Id", "SalePrice"), sep =',', row.names = FALSE)
write.table(prediction_LASSO_1se, file="prediction_LASSO_1se.csv",col.names = c("Id", "SalePrice"), sep =',', row.names = FALSE)
```

### Regression Tree

```{r}
tree.SalePrice <-tree(SalePrice~., data = train_df)
#summary(tree.SalePrice)
plot(tree.SalePrice)
text(tree.SalePrice, pretty=1)
```

```{r,  eval = FALSE}
cv.SalePrice <-cv.tree(tree.SalePrice, K = 10)
plot(cv.SalePrice$size, cv.SalePrice$dev, type="b")
## In this case, the most complex tree is selected by cross-validation
prune.SalePrice <-prune.tree(tree.SalePrice, best=10)
plot(prune.SalePrice)
text(prune.SalePrice, pretty=1)
cv.SalePrice$dev # no PRUNING DONE
```

```{r,  eval = FALSE}
prediction_TREE_log <- predict(prune.SalePrice ,test_df)
prediction_TREE <- exp(prediction_TREE_log)
```

```{r,  eval = FALSE}
prediction_TREE <- cbind(Id = rownames(X_test), SalesPrice = prediction_TREE)
```

```{r, eval = FALSE}
write.table(prediction_TREE, file="prediction_TREE.csv",col.names = c("Id", "SalePrice"), sep =',', row.names = FALSE)
```

### Bagging

```{r,  eval = FALSE}
set.seed(1)
bag.SalePrice <-randomForest(SalePrice~., data=train_df, mtry= 79, importance=TRUE)
bag.SalePrice
```

```{r,  eval = FALSE}
prediction_bag_log <- predict(bag.SalePrice, newdata = test_df)
prediction_bag <- exp(prediction_bag_log)
```

```{r,  eval = FALSE}
prediction_bag <- cbind(Id = rownames(X_test), SalesPrice = prediction_bag)
```

```{r,  eval = FALSE}
write.table(prediction_bag, file="prediction_bag.csv",col.names = c("Id", "SalePrice"), sep =',', row.names = FALSE)
```

### Random Forest $m = \frac{p}{3}$

```{r, eval = FALSE}
RF_p3.SalePrice <-randomForest(SalePrice~., data=train_df, mtry = 26, importance=TRUE)
```

```{r, eval = FALSE}
prediction_RF_p3_log <- predict(RF_p3.SalePrice, newdata = test_df)
prediction_RF_p3 <- exp(prediction_RF_p3_log)
```

```{r, eval = FALSE}
prediction_RF_p3 <- cbind(Id = rownames(X_test), SalesPrice = prediction_RF_p3)
```

```{r, eval = FALSE}
write.table(prediction_RF_p3, file="prediction_RF_p3.csv",col.names = c("Id", "SalePrice"), sep =',', row.names = FALSE)
```

### Random Forest $m = \sqrt(p)$

```{r,  eval = FALSE}
## We could change the number of trees grown by randomForest() using ntree argument
RF.SalePrice <-randomForest(SalePrice~., data=train_df, importance=TRUE)
```

```{r,  eval = FALSE}
prediction_RF_log <- predict(RF.SalePrice, newdata = test_df)
prediction_RF <- exp(prediction_RF_log)
```

```{r,  eval = FALSE}
prediction_RF <- cbind(Id = rownames(X_test), SalesPrice = prediction_RF)
```

```{r, eval = FALSE}
write.table(prediction_RF, file="prediction_RF.csv",col.names = c("Id", "SalePrice"), sep =',', row.names = FALSE)
```

### Tuning Random Forest - selecting 'm'

```{r,  eval = FALSE}
x_train_df <- train_df
x_train_df$SalePrice <- NULL
```

```{r,  eval = FALSE}
results <- rfcv(x_train_df, y_train, cv.fold=10, scale="log", step=0.5)
```
```{r, eval = FALSE}
results$error.cv

```

```{r,  eval = FALSE}
## We could change the number of trees grown by randomForest() using ntree argument
RF.SalePrice_tuned <-randomForest(SalePrice~., data=train_df, importance=TRUE, ntree = 1000, mtry=38)
```

```{r,  eval = FALSE}
prediction_RF_tuned_log <- predict(RF.SalePrice_tuned, newdata = test_df)
prediction_RF_tuned <- exp(prediction_RF_tuned_log)

```
```{r,  eval = FALSE}
prediction_RF_tuned <- cbind(Id = rownames(X_test), SalesPrice = prediction_RF_tuned)
```
```{r, eval = FALSE}
write.table(prediction_RF_tuned, file="prediction_RF_tuned.csv",col.names = c("Id", "SalePrice"), sep =',', row.names = FALSE)
```

### Gradient Boosting Model - library (gbm)

```{r}
set.seed(1)
boost.train = gbm(SalePrice ~. , data=train_df,
                  distribution = "gaussian",
                  n.trees = 1000, 
                  shrinkage = 0.05,
                  interaction.depth = 2, 
                  bag.fraction = 0.66,
                  cv.folds = 10, 
                  verbose = FALSE,
                  n.cores = 8)

min(boost.train$cv.error)

```
```{r}
#attributes(boost.train)
bestTreeForPrediction = gbm.perf(boost.train)
```

```{r, eval = FALSE}
prediction_BOOST_log <- predict(boost.train, test_df)
prediction_BOOST <- exp(prediction_BOOST_log)
```

```{r,  eval = FALSE}
prediction_BOOST <- cbind(Id = rownames(X_test), SalesPrice = prediction_BOOST)
```
```{r, eval = FALSE}
write.table(prediction_BOOST, file="prediction_BOOST.csv",col.names = c("Id", "SalePrice"), sep =',', row.names = FALSE)
```

### XGBoost

```{r,  eval = FALSE}
#setup
library(parallel) #checking the number of cores
detectCores() #=> 8
dtrain <- xgb.DMatrix(X_train, label = y_train)
```

Cross Validation in XGBoost - CV  
Source: [StackOverflow](https://stackoverflow.com/questions/35050846/xgboost-in-r-how-does-xgb-cv-pass-the-optimal-parameters-into-xgb-train)
```{r, eval = FALSE}
best_param2 = list()
best_rmse = Inf
best_rmse_index = 0
best_seednumber = 1234

for (iter in 1:20) {
    param <- list(objective = "reg:linear",
          max_depth = sample(2:6, 1),
          eta = runif(1, .01, .05),
          gamma = runif(1, 0.0, 0.013), 
          subsample = runif(1, .7, .8),
          colsample_bytree = runif(1, .6, .7), 
          min_child_weight = sample(1:3, 1)
          )
    cv.nround = 1000
    cv.nfold = 10
    seed.number = sample.int(10000, 1)[[1]]
    set.seed(seed.number)
    mdcv <- xgb.cv(data=dtrain, params = param, nthread=8, 
                    nfold=cv.nfold, nrounds=cv.nround,
                    verbose = F, early.stop.rounds=8, maximize=FALSE)

    min_rmse = min(mdcv$evaluation_log[,test_rmse_mean])
    min_rmse_index = which.min(mdcv$evaluation_log[,test_rmse_mean])
    

    if (min_rmse < best_rmse) {
        best_rmse = min_rmse
        best_rmse_index = min_rmse_index
        best_seednumber = seed.number
        best_param2 = param
    }
    print(iter)
}
```

```{r, eval = FALSE}
print(best_param) # rmse = 0.12567
```
```{r, eval = FALSE}
nround = best_rmse_index
set.seed(best_seednumber)
model_XGB_tune2 <- xgb.train(data=dtrain, params=best_param2, nrounds=nround, nthread=8)
```

```{r,  eval = FALSE}
prediction_XGB_tune2_log <- predict(model_XGB_tune2, X_test)
prediction_XGB_tune2 <- exp(prediction_XGB_tune2_log)
```

```{r,  eval = FALSE}
prediction_XGB_tune2 <- cbind(Id = rownames(X_test), SalesPrice = prediction_XGB_tune2)
```

```{r,  eval = FALSE}
write.table(prediction_XGB_tune2, file="prediction_XBG_tune2.csv",col.names = c("Id", "SalePrice"), sep =',', row.names = FALSE)
```