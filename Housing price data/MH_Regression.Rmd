---
title: "Regression"
author: "Michal Heydel"
date: "30 November 2017"
output: html_document
---
```{r, warning=FALSE, message=FALSE}
library(boot)
library(leaps)
library(tree) # Normal Tree
library(randomForest) # random Forest
library(xgboost) # boosting trees
library(Matrix) #?
library(methods) #?
library(caret) # for tuning xgboost

```



```{r Generating matrix from the data frames, no intercept}
X_train<- model.matrix(SalePrice~.-1, data = train_df)
y_train <- train_df$SalePrice
X_test <- model.matrix(~.-1, data=test_df)
```



```{r}
#Check is some there are more levels in some of the categorical factors in the testing compared to the training
for(i in colnames(train_df[,sapply(train_df, is.factor)])){
  if (length(levels(train_df[,i])) < length(levels(test_df[,i]))) {
    print(i)
    print(levels(train_df[,i]))
    print(levels(test_df[,i]))
  }
}

#150 is appears in the testing data, but doesn't appear in the training data
```


####REGRESSION------------------------------------------------------------------------------

#regression with all the parameters - just as a benchmark

```{r}
df[df$MSSubClass == 150,] 
df[df$MSSubClass == 150,"MSSubClass"] <- 120
```





```{r, eval = FALSE}
lm_fit_all = lm(SalePrice ~., data = train_df) 
#summary(lm_fit_all)
```

```{r, warning=FALSE, message=FALSE,  eval = FALSE}
prediction_LR_ALL <- predict(lm_fit_all, test_df, type="response")
```

```{r,  eval = FALSE}
prediction_LR_ALL <- as.data.frame(prediction_LR_ALL)
prediction_LR_ALL <- cbind(Id = rownames(prediction_LR_ALL), SalesPrice = prediction_LR_ALL)
```

```{r,  eval = FALSE}
write.table(prediction_LR_ALL, file="prediction_LR_ALL.csv",col.names = c("Id","SalePrice"), sep =',', row.names = FALSE)
```


####LASSO -------------------------------------------------------------------------------------

```{r,  eval = FALSE}
cv.lasso <-cv.glmnet(X_train, y_train, nfolds = 10, alpha = 1)
plot(cv.lasso)
#here is an example why should we apply the one standard deviation error rule!
```
```{r,  eval = FALSE}
penalty_min <- cv.lasso$lambda.min #optimal lambda
penalty_1se <- cv.lasso$lambda.1se # 1 Standard Error Apart
fit.lasso_min <-glmnet(X_train, y_train, alpha = 1, lambda = penalty_min) #estimate the model with min lambda
fit.lasso_1se <-glmnet(X_train, y_train, alpha = 1, lambda = penalty_1se) #estimate the model with 1se apart
```

```{r,  eval = FALSE}
prediction_LASSO_min_log <- predict(fit.lasso_min, X_test)
prediction_LASSO_1se_log <- predict(fit.lasso_1se, X_test)

prediction_LASSO_min <- exp(prediction_LASSO_min_log)
prediction_LASSO_1se <- exp(prediction_LASSO_1se_log)
```

```{r, eval = FALSE}
prediction_LASSO_min <-cbind(Id = rownames(prediction_LASSO_min), SalesPrice = prediction_LASSO_min)
prediction_LASSO_1se <- cbind(Id = rownames(prediction_LASSO_1se), SalesPrice = prediction_LASSO_1se)
```

```{r, eval = False}
write.table(prediction_LASSO_min, file="prediction_LASSO_min.csv",col.names = c("Id", "SalePrice"), sep =',', row.names = FALSE)
write.table(prediction_LASSO_1se, file="prediction_LASSO_1se.csv",col.names = c("Id", "SalePrice"), sep =',', row.names = FALSE)
```

### REGRESSION TREE - SIMPLE---------------------------------------------------------------------------

```{r,  eval = FALSE}
## Regression Tree
tree.SalePrice <-tree(SalePrice~., data =train_df)
summary(tree.SalePrice)
plot(tree.SalePrice)
text(tree.SalePrice, pretty=1)
```
```{r,  eval = FALSE}
cv.SalePrice <-cv.tree(tree.SalePrice, K = 10)
plot(cv.SalePrice$size, cv.SalePrice$dev, type="b")
## In this case, the most complex tree is selected by cross-validation
## However, if we wish to prune the tree, we could do so as follows using prune.tree() function
prune.SalePrice <-prune.tree(tree.SalePrice, best=11)
plot(prune.SalePrice)
text(prune.SalePrice, pretty=1)
cv.SalePrice$dev # no PRUNING DONE
```
```{r,  eval = FALSE}
prediction_TREE <- predict(prune.SalePrice ,test_df)
```
```{r,  eval = FALSE}
prediction_TREE <- cbind(Id = rownames(prediction_TREE), SalesPrice = prediction_TREE)
```
```{r, eval = FALSE}
write.table(prediction_TREE, file="prediction_TREE.csv",col.names = c("Id", "SalePrice"), sep =',', row.names = FALSE)
```





###BAGGING-------------------------------------------------------------------------------



```{r,  eval = FALSE}
##Bagging
library(randomForest)
set.seed(1)
## Recall that bagging is simply a special case of a random forest with m=p, here we use mtry=13, i.e. bagging would be done
bag.SalePrice <-randomForest(SalePrice~., data=train_df, mtry= 79, importance=TRUE)
bag.SalePrice
```

```{r,  eval = FALSE}
prediction_bag <- predict(bag.SalePrice, newdata = test_df)
```

```{r,  eval = FALSE}
prediction_bag <- cbind(Id = rownames(prediction_bag), SalesPrice = prediction_bag)
prediction_bag <- cbind(Id = rownames(prediction_bag), SalesPrice = prediction_bag)
```
```{r,  eval = FALSE}
write.table(prediction_bag, file="prediction_bag.csv",col.names = c("Id", "SalePrice"), sep =',', row.names = FALSE)
```

#Random Forest m= p/3 ------------------------------------------------------------------------------------------

```{r, eval = FALSE}
## We could change the number of trees grown by randomForest() using ntree argument
RF_p3.SalePrice <-randomForest(SalePrice~., data=train_df, mtry = 26, importance=TRUE)
```

```{r, eval = FALSE}
prediction_RF_p3_log <- predict(RF_p3.SalePrice, newdata = test_df)
prediction_RF_p3 <- exp(prediction_RF_p3_log)
```

```{r, eval = FALSE}
prediction_RF_p3 <- cbind(Id = names(prediction_RF_p3), SalesPrice = prediction_RF_p3)
```

```{r, eval = FALSE}
write.table(prediction_RF_p3, file="prediction_RF_p3.csv",col.names = c("Id", "SalePrice"), sep =',', row.names = FALSE)
```

#Random Forest ------------------------------------------------------------------------------------------

```{r,  eval = FALSE}
## We could change the number of trees grown by randomForest() using ntree argument
RF.SalePrice <-randomForest(SalePrice~., data=train_df, importance=TRUE)
```

```{r,  eval = FALSE}
prediction_RF <- predict(RF.SalePrice, newdata = test_df)
```
```{r,  eval = FALSE}
prediction_RF <- cbind(Id = rownames(prediction_RF), SalesPrice = prediction_RF)
prediction_RF <- cbind(Id = rownames(prediction_RF), SalesPrice = prediction_RF)
```
```{r, eval = False}
write.table(prediction_RF, file="prediction_RF.csv",col.names = c("Id", "SalePrice"), sep =',', row.names = FALSE)
```


###Tuning randomForest---------------------------------------------------------------------------------



```{r,  eval = FALSE}
x_train_df <- train_df
x_train_df$SalePrice <- NULL
```

```{r,  eval = FALSE}
results <- rfcv(x_train_df, y_train, cv.fold=10, scale="log", step=0.7)
```


```{r,  eval = FALSE}
## We could change the number of trees grown by randomForest() using ntree argument
RF.SalePrice_tuned <-randomForest(SalePrice~., data=train_df, importance=TRUE, ntree = 1000, mtry=36)
```

```{r,  eval = FALSE}
prediction_RF_tuned_log <- predict(RF.SalePrice_tuned, newdata = test_df)
prediction_RF_tuned <- exp(prediction_RF_tuned_log)

```
```{r,  eval = FALSE}
prediction_RF_tuned <- cbind(Id = names(prediction_RF_tuned), SalesPrice = prediction_RF_tuned)
```
```{r, eval = False}
write.table(prediction_RF_tuned, file="prediction_RF_tuned.csv",col.names = c("Id", "SalePrice"), sep =',', row.names = FALSE)
```


```{r, eval = FALSE}
prediction_RF_tune_log <- predict(bestmtry, newdata = test_df)
prediction_RF_tune <- exp(prediction_RF_tune_log)
```

```{r, eval = FALSE}
prediction_RF_tune <- cbind(Id = names(prediction_RF_tune), SalesPrice = prediction_RF_p3)
```

```{r, eval = FALSE}
write.table(prediction_RF_p3, file="prediction_RF_p3.csv",col.names = c("Id", "SalePrice"), sep =',', row.names = FALSE)
```


####GBM ---------------------------------------------------------------------------------------


```{r, warning= FALSE}
library(gbm)
```

```{r}
set.seed(1)
boost.train = gbm(SalePrice ~. , data=train_df,
                  distribution = "gaussian",
                  n.trees = 1000, 
                  shrinkage = 0.05,
                  interaction.depth = 5, 
                  bag.fraction = 0.66,
                  cv.folds = 10, 
                  verbose = FALSE,
                  n.cores = 8)

min(boost.train$cv.error)

```
```{r}
#attributes(boost.train)
bestTreeForPrediction = gbm.perf(boost.train)
```



```{r}
prediction_BOOST_log <- predict(boost.train, test_df)
prediction_BOOST <- exp(prediction_BOOST_log)
```

```{r,  eval = FALSE}
prediction_BOOST <- cbind(Id = rownames(X_test), SalesPrice = prediction_BOOST)
```
```{r, eval = False}
write.table(prediction_BOOST, file="prediction_BOOST.csv",col.names = c("Id", "SalePrice"), sep =',', row.names = FALSE)
```


######xgboost-----------------------------------------------------------------------------------------------

```{r,  eval = FALSE}
#setup
library(parallel)
detectCores() #=> 8
dtrain <- xgb.DMatrix(X_train, label = y_train)
```


###XGBoost - CV-------------------------------------------------
#####https://stackoverflow.com/questions/35050846/xgboost-in-r-how-does-xgb-cv-pass-the-optimal-parameters-into-xgb-train
```{r, eval = FALSE}
best_param2 = list()
best_rmse = Inf
best_rmse_index = 0
best_seednumber = 1234

for (iter in 1:20) {
    param <- list(objective = "reg:linear",
          max_depth = sample(1:10, 1),
          eta = runif(1, .01, .05),
          gamma = runif(1, 0.0, 0.013), 
          subsample = runif(1, .7, .8),
          colsample_bytree = runif(1, .6, .7), 
          min_child_weight = sample(1:10, 1)
          )
    cv.nround = 1000
    cv.nfold = 10
    seed.number = sample.int(10000, 1)[[1]]
    set.seed(seed.number)
    mdcv <- xgb.cv(data=dtrain, params = param, nthread=8, 
                    nfold=cv.nfold, nrounds=cv.nround,
                    verbose = F, early.stop.rounds=8, maximize=FALSE)

    min_rmse = min(mdcv$evaluation_log[,test_rmse_mean])
    min_rmse_index = which.min(mdcv$evaluation_log[,test_rmse_mean])
    

    if (min_rmse < best_rmse) {
        best_rmse = min_rmse
        best_rmse_index = min_rmse_index
        best_seednumber = seed.number
        best_param2 = param
    }
    print(iter)
}
```

```{r}
print(best_param2) # rmse = 0.12434
#print(best_param) #rmse.cv.error = 0.12567
```
```{r}
nround = best_rmse_index
set.seed(best_seednumber)
model_XGB_tune2 <- xgb.train(data=dtrain, params=best_param2, nrounds=nround, nthread=8)
```

```{r,  eval = FALSE}
prediction_XGB_tune2_log <- predict(model_XGB_tune2, X_test)
prediction_XGB_tune2 <- exp(prediction_XGB_tune2_log)
```


```{r,  eval = FALSE}
prediction_XGB_tune2 <- cbind(Id = rownames(X_test), SalesPrice = prediction_XGB_tune2)
```

```{r,  eval = FALSE}
write.table(prediction_XGB_tune2, file="prediction_XBG_tune2.csv",col.names = c("Id", "SalePrice"), sep =',', row.names = FALSE)
```




















```{r, eval = FALSE}
###CODE TEST
set.seed(7862)
params <- list("objective" = "reg:linear",
              "max_depth"= 6,
              "eta" = 0.02,
              "subsample"=0.8,
              "colsample_bytree" = 0.8,
              "min_child_weight" = 1,
              "gamma" = 0
              )


cv.nround = 1000
cv.nfold = 5

history <- xgb.cv(data = dtrain, params = best_param1, nthread = 8, nfold = cv.nfold , early.stop.round=8, nrounds = cv.nround, maximize=FALSE)


print(history)
```

