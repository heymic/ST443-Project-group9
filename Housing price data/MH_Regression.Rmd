---
title: "Regression"
author: "Michal Heydel"
date: "30 November 2017"
output: html_document
---
```{r, warning=FALSE, message=FALSE}
library(boot)
library(leaps)
library(tree)
library(MASS)
#library(Metrics)
#library(corrplot)
library(randomForest)
#library(lars)
library(ggplot2)
library(xgboost)
library(Matrix)
library(methods)
library(caret)

```

```{r}
for(i in colnames(df[,sapply(df, is.character)])){
    df[,i] <- as.factor(df[,i])
}
str(df)
```
```{r}
train_df <- df[df$isTrain==1,]
test_kaggle <- df[df$isTrain==0,]

train_df$isTrain <- NULL
test_kaggle$isTrain <- NULL
test_kaggle$SalePrice <- NULL
```




```{r, eval = False}
for(i in colnames(train_df[,sapply(train_df, is.character)])){
    train_df[,i] <- as.factor(train_df[,i])
}
```

```{r, eval = False}
for(i in colnames(test_kaggle[,sapply(test_kaggle, is.character)])){
    test_kaggle[,i] <- as.factor(test_kaggle[,i])
}
```

```{r}
#Check is some there are more levels in some of the categorical factors in the testing compared to the training
for(i in colnames(train_df[,sapply(train_df, is.factor)])){
  if (length(levels(train_df[,i])) < length(levels(test_kaggle[,i]))) {
    print(i)
  }
}

```


####REGRESSION------------------------------------------------------------------------------

#regression with all the parameters - just as a benchmark

```{r}
lm_fit_all = lm(SalePrice ~., data = train_df) 
#summary(lm_fit_all)
```

```{r, warning=FALSE, message=FALSE}
prediction_LR_ALL <- predict(lm_fit_all, test_kaggle, type="response")
```

```{r}
prediction_LR_ALL <- as.data.frame(prediction_LR_ALL)
prediction_LR_ALL <- cbind(Id = rownames(prediction_LR_ALL), SalesPrice = prediction_LR_ALL)
```

```{r, eval = False}
write.table(prediction_LR_ALL, file="prediction_LR_ALL.csv",col.names = c("Id","SalePrice"), sep =',', row.names = FALSE)
```


####LASSO -------------------------------------------------------------------------------------

```{r}
## Lasso regression

x <- model.matrix(SalePrice~.-1, data = train_df)
y <- train$SalePrice
```


```{r}
cv.lasso <-cv.glmnet(x, y, nfolds = 10, alpha = 1)
plot(cv.lasso)
#here is an example why should we apply the one standard deviation error rule!
```
```{r}
penalty_min <- cv.lasso$lambda.min #optimal lambda
penalty_1se <- cv.lasso$lambda.1se # 1 Standard Error Apart
fit.lasso_min <-glmnet(x, y, alpha = 1, lambda = penalty_min ) #estimate the model with min lambda
fit.lasso_1se <-glmnet(x, y, alpha = 1, lambda = penalty_1se ) #estimate the model with 1se apart
```

```{r}
test_kaggle1 <- model.matrix(~., data=test_kaggle)
prediction_LASSO_min <- predict(fit.lasso_min, test_kaggle1)
prediction_LASSO_1se <- predict(fit.lasso_1se, test_kaggle1)
```

```{r}
prediction_LASSO_min <-cbind(Id = rownames(prediction_LASSO_min), SalesPrice = prediction_LASSO_min)
prediction_LASSO_1se <- cbind(Id = rownames(prediction_LASSO_1se), SalesPrice = prediction_LASSO_1se)
```

```{r, eval = False}
write.table(prediction_LASSO_min, file="prediction_LASSO_min.csv",col.names = c("Id", "SalePrice"), sep =',', row.names = FALSE)
write.table(prediction_LASSO_1se, file="prediction_LASSO_1se.csv",col.names = c("Id", "SalePrice"), sep =',', row.names = FALSE)
```

### REGRESSION TREE - SIMPLE---------------------------------------------------------------------------

```{r}
## Regression Tree
tree.SalePrice <-tree(SalePrice~., data =train_df)
summary(tree.SalePrice)
plot(tree.SalePrice)
text(tree.SalePrice, pretty=1)
```
```{r}
cv.SalePrice <-cv.tree(tree.SalePrice, K = 10)
plot(cv.SalePrice$size, cv.SalePrice$dev, type="b")
## In this case, the most complex tree is selected by cross-validation
## However, if we wish to prune the tree, we could do so as follows using prune.tree() function
prune.SalePrice <-prune.tree(tree.SalePrice, best=11)
plot(prune.SalePrice)
text(prune.SalePrice, pretty=1)
cv.SalePrice$dev # no PRUNING DONE
```
```{r}
prediction_TREE <- predict(prune.SalePrice ,test_kaggle)
```
```{r}
prediction_TREE <- cbind(Id = rownames(prediction_TREE), SalesPrice = prediction_TREE)
```
```{r, eval = False}
write.table(prediction_TREE, file="prediction_TREE.csv",col.names = c("Id", "SalePrice"), sep =',', row.names = FALSE)
```





###BAGGING-------------------------------------------------------------------------------



```{r}
##Bagging
library(randomForest)
set.seed(1)
## Recall that bagging is simply a special case of a random forest with m=p, here we use mtry=13, i.e. bagging would be done
bag.SalePrice <-randomForest(SalePrice~., data=train_df, mtry= 79, importance=TRUE)
bag.SalePrice
```
```{r}
prediction_bag <- predict(bag.SalePrice, newdata = test_kaggle)
```
```{r}
prediction_bag <- cbind(Id = rownames(prediction_bag), SalesPrice = prediction_bag)
prediction_bag <- cbind(Id = rownames(prediction_bag), SalesPrice = prediction_bag)
```
```{r, eval = False}
write.table(prediction_bag, file="prediction_bag.csv",col.names = c("Id", "SalePrice"), sep =',', row.names = FALSE)
```

#Random Forest m= p/3 ------------------------------------------------------------------------------------------

```{r}
## We could change the number of trees grown by randomForest() using ntree argument
RF_p3.SalePrice <-randomForest(SalePrice~., data=train_df, mtry = 26, importance=TRUE)
```

```{r}
prediction_RF_p3 <- predict(RF_p3.SalePrice, newdata = test_kaggle)
```
```{r}
prediction_RF_p3 <- cbind(Id = rownames(prediction_RF_p3), SalesPrice = prediction_RF_p3)
prediction_RF_p3 <- cbind(Id = rownames(prediction_RF_p3), SalesPrice = prediction_RF_p3)
```
```{r, eval = False}
write.table(prediction_RF_p3, file="prediction_RF_p3.csv",col.names = c("Id", "SalePrice"), sep =',', row.names = FALSE)
```

#Random Forest ------------------------------------------------------------------------------------------

```{r}
## We could change the number of trees grown by randomForest() using ntree argument
RF.SalePrice <-randomForest(SalePrice~., data=train_df, importance=TRUE)
```

```{r}
prediction_RF <- predict(RF.SalePrice, newdata = test_kaggle)
```
```{r}
prediction_RF <- cbind(Id = rownames(prediction_RF), SalesPrice = prediction_RF)
prediction_RF <- cbind(Id = rownames(prediction_RF), SalesPrice = prediction_RF)
```
```{r, eval = False}
write.table(prediction_RF, file="prediction_RF.csv",col.names = c("Id", "SalePrice"), sep =',', row.names = FALSE)
```



######xgboost-----------------------------------------------------------------------------------------------



```{r}
#eta = 0.3 by default
#gamma = 0 by default,
#max_depth [default=6]

params <- list("objective" = "reg:linear",
              "nthread" = 8,
              "eta" = 0.25,
              "subsample"=0.8,
              "gamma" = 0.1,
              "max_depth"= 6,
              "colsample_bytree" = 1)

model_xg <- xgboost(param=params, data = x, label = y, nrounds=150)

```

```{r}
prediction_XBG <- predict(model_xg, test_kaggle1)
```


```{r}
prediction_XBG <- cbind(Id = rownames(prediction_bag), SalesPrice = prediction_XBG)
```

```{r, eval = False}
write.table(prediction_XBG, file="prediction_XBG.csv",col.names = c("Id", "SalePrice"), sep =',', row.names = FALSE)
```








```{r}
train_Data <- xgb.DMatrix(data = x, label = y)

cv.ctrl <- trainControl(method = "repeatedcv", repeats = 1,number = 3)

xgb.grid <- expand.grid(nrounds = 500,
                        max_depth = c(6,10),
                        eta = c(0.01,0.3, 1),
                        gamma = c(0.0, 0.2, 1),
                        colsample_bytree = c(0.5,0.8, 1),
                        min_child_weight=c(1,3),
                        subsample =c(0.8)
)

xgb_tune <-train(x,y,
                 method="xgbTree",
                 metric = "RMSE",
                 trControl=cv.ctrl,
                 tuneGrid=xgb.grid, nthreads = 8
)
```




```{r}
# set up the cross-validated hyper-parameter search
xgb_grid_1 = expand.grid(nrounds = 500,
                        max_depth = seq(6,10),
                        eta = c(0.01,0.3, 1),
                        gamma = c(0.0, 0.2, 1),
                        colsample_bytree = c(0.5,0.8, 1),
                        min_child_weight=seq(1,10),
                        nthread = 8
                        )
 
# pack the training control parameters
xgb_trcontrol_1 = trainControl(method = "cv",number = 5)
 
# train the model for each parameter combination in the grid,
#   using CV to evaluate
xgb_train_1 = train(x, y, trControl = xgb_trcontrol_1, tuneGrid = xgb_grid_1, method = "xgbTree",metric = "RMSE")

 

```
```{r}
# scatter plot of the AUC against max_depth and eta
ggplot(xgb_train_1$results, aes(x = as.factor(eta), y = max_depth, size = ROC, color = ROC)) +
geom_point() +
theme_bw() +
scale_size_continuous(guide = "none")
```


