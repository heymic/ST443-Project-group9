mdcv <- xgb.cv(data=dtrain, params = param, nthread=8,
nfold=cv.nfold, nrounds=cv.nround,
verbose = F, early.stop.rounds=8, maximize=FALSE)
min_rmse = min(mdcv$evaluation_log[,test_rmse_mean])
min_rmse_index = which.min(mdcv$evaluation_log[,test_rmse_mean])
if (min_rmse < best_rmse) {
best_rmse = min_rmse
best_rmse_index = min_rmse_index
best_seednumber = seed.number
best_param2 = param
}
print(iter)
}
print(best_param2) # rmse = 0.12608
#print(best_param) #rmse.cv.error = 0.12567
nround = best_rmse_index
set.seed(best_seednumber)
model_XGB_tune2 <- xgb.train(data=dtrain, params=best_param2, nrounds=nround, nthread=8)
nround = best_rmse_index
set.seed(best_seednumber)
model_XGB_tune2 <- xgb.train(data=dtrain, params=best_param2, nrounds=nround, nthread=8)
prediction_XGB_tune2_log <- predict(model_XGB_tune2, X_test)
prediction_XGB_tune2 <- exp(prediction_XGB_tune2_log)
prediction_XGB_tune2 <- cbind(Id = rownames(X_test), SalesPrice = prediction_XGB_tune2)
write.table(prediction_XGB_tune2, file="prediction_XBG_tune2.csv",col.names = c("Id", "SalePrice"), sep =',', row.names = FALSE)
#colSums(sapply(df, is.na))
sum(is.na(df)) == 1459
print(best_param2) # rmse = 0.12608
#print(best_param) #rmse.cv.error = 0.12567
nround = best_rmse_index
set.seed(best_seednumber)
model_XGB_tune2 <- xgb.train(data=dtrain, params=best_param2, nrounds=nround, nthread=8)
prediction_XGB_tune2_log <- predict(model_XGB_tune2, X_test)
prediction_XGB_tune2 <- exp(prediction_XGB_tune2_log)
prediction_XGB_tune2 <- cbind(Id = rownames(X_test), SalesPrice = prediction_XGB_tune2)
write.table(prediction_XGB_tune2, file="prediction_XBG_tune2.csv",col.names = c("Id", "SalePrice"), sep =',', row.names = FALSE)
set.seed(1)
boost.train = gbm(SalePrice ~. , data=train_df,
distribution = "gaussian",
n.trees = 1000,
shrinkage = 0.05,
interaction.depth = 5,
bag.fraction = 0.66,
cv.folds = 10,
verbose = FALSE,
n.cores = 8)
min(boost.train$cv.error)
set.seed(1)
boost.train = gbm(SalePrice ~. , data=train_df,
distribution = "gaussian",
n.trees = 1000,
shrinkage = 0.05,
interaction.depth = 5,
bag.fraction = 1,
cv.folds = 10,
verbose = FALSE,
n.cores = 8)
min(boost.train$cv.error)
set.seed(1)
boost.train = gbm(SalePrice ~. , data=train_df,
distribution = "gaussian",
n.trees = 1000,
shrinkage = 0.05,
interaction.depth = 5,
bag.fraction = 0.4,
cv.folds = 10,
verbose = FALSE,
n.cores = 8)
min(boost.train$cv.error)
set.seed(1)
boost.train = gbm(SalePrice ~. , data=train_df,
distribution = "gaussian",
n.trees = 1000,
shrinkage = 0.02,
interaction.depth = 5,
bag.fraction = 0.66,
cv.folds = 10,
verbose = FALSE,
n.cores = 8)
min(boost.train$cv.error)
set.seed(1)
boost.train = gbm(SalePrice ~. , data=train_df,
distribution = "gaussian",
n.trees = 1000,
shrinkage = 0.01,
interaction.depth = 5,
bag.fraction = 0.66,
cv.folds = 10,
verbose = FALSE,
n.cores = 8)
min(boost.train$cv.error)
set.seed(1)
boost.train = gbm(SalePrice ~. , data=train_df,
distribution = "gaussian",
n.trees = 1000,
shrinkage = 0.05,
interaction.depth = 2,
bag.fraction = 0.66,
cv.folds = 10,
verbose = FALSE,
n.cores = 8)
min(boost.train$cv.error)
set.seed(1)
boost.train = gbm(SalePrice ~. , data=train_df,
distribution = "gaussian",
n.trees = 1000,
shrinkage = 0.05,
interaction.depth = 1,
bag.fraction = 0.66,
cv.folds = 10,
verbose = FALSE,
n.cores = 8)
min(boost.train$cv.error)
set.seed(1)
boost.train = gbm(SalePrice ~. , data=train_df,
distribution = "gaussian",
n.trees = 1000,
shrinkage = 0.05,
interaction.depth = 2,
bag.fraction = 0.66,
cv.folds = 10,
verbose = FALSE,
n.cores = 8)
min(boost.train$cv.error)
set.seed(1)
boost.train = gbm(SalePrice ~. , data= X_train,
distribution = "gaussian",
n.trees = 1000,
shrinkage = 0.05,
interaction.depth = 2,
bag.fraction = 0.66,
cv.folds = 10,
verbose = FALSE,
n.cores = 8)
gbm
?gbm
set.seed(1)
boost.train = gbm.fit(x = X_train, y = y_train,
distribution = "gaussian",
n.trees = 1000,
shrinkage = 0.05,
interaction.depth = 2,
bag.fraction = 0.66,
cv.folds = 10,
verbose = FALSE,
n.cores = 8)
set.seed(1)
boost.train = gbm.fit( X_train, y_train,
distribution = "gaussian",
n.trees = 1000,
shrinkage = 0.05,
interaction.depth = 2,
bag.fraction = 0.66,
cv.folds = 10,
verbose = FALSE,
n.cores = 8)
set.seed(1)
boost.train = gbm(SalePrice ~. , data= X_train,
distribution = "gaussian",
n.trees = 1000,
shrinkage = 0.05,
interaction.depth = 2,
bag.fraction = 0.66,
cv.folds = 10,
verbose = FALSE,
n.cores = 8)
set.seed(1)
boost.train = gbm(SalePrice ~. , data=train_df,
distribution = "gaussian",
n.trees = 1000,
shrinkage = 0.05,
interaction.depth = 2,
bag.fraction = 0.66,
cv.folds = 10,
verbose = FALSE,
n.cores = 8)
min(boost.train$cv.error)
results <- rfcv(x_train_df, y_train, cv.fold=10, scale="log", step=0.9)
results$error.cv
?rfcv
?randomForest
## We could change the number of trees grown by randomForest() using ntree argument
RF.SalePrice_tuned <-randomForest(SalePrice~., data=train_df, importance=TRUE, ntree = 1000, mtry=22)
prediction_RF_tuned_log <- predict(RF.SalePrice_tuned, newdata = test_df)
prediction_RF_tuned <- exp(prediction_RF_tuned_log)
prediction_RF_tuned <- cbind(Id = rownames(X_test), SalesPrice = prediction_RF_tuned)
write.table(prediction_RF_tuned, file="prediction_RF_tuned.csv",col.names = c("Id", "SalePrice"), sep =',', row.names = FALSE)
cv.ridge <-cv.glmnet(X_train, y_train, nfolds = 10, alpha = 0)
library(dplyr)
Mode <- function(x) {
ux <- unique(x)
ux[which.max(tabulate(match(x, ux)))]
}
setwd("C:/Users/Michal/Documents/01- Master Degree/GitHub/ST443-Project-group9/Housing price data")
getwd()
train_raw = read.csv("train.csv", row.names = "Id", stringsAsFactors=FALSE)
testing_raw = read.csv("test.csv", row.names = "Id", stringsAsFactors=FALSE)
#combining train and test data for quicker data prep
testing_raw$SalePrice <- NA
train_raw$isTrain <- 1
testing_raw$isTrain <- 0
df <- rbind(train_raw,testing_raw)
colSums(sapply(df, is.na))
df[,c('PoolQC','PoolArea')] %>%
group_by(PoolQC) %>%
summarise(mean = mean(PoolArea), counts = n())
df[(df$PoolArea > 0) & is.na(df$PoolQC),c('PoolQC','PoolArea')]
df[2421,'PoolQC'] = 'Ex'
df[2504,'PoolQC'] = 'Ex'
df[2600,'PoolQC'] = 'Fa'
df$PoolQC[is.na(df$PoolQC)] = 'None'
garage.cols <- c('GarageArea', 'GarageCars', 'GarageQual', 'GarageFinish', 'GarageCond', 'GarageType')
#df[is.na(df$GarageCond),garage.cols]
#length(which(df$GarageYrBlt == df$YearBuilt))
df[(df$GarageArea > 0) & is.na(df$GarageYrBlt), c(garage.cols, 'GarageYrBlt')]
df$GarageYrBlt[2127] <- df$YearBuilt[2127]
df[2127, 'GarageQual'] <- Mode(df$GarageQual)
df[2127, 'GarageFinish'] <- Mode(df$GarageFinish)
df[2127, 'GarageCond'] <- Mode(df$GarageCond)
df$GarageYrBlt[which(is.na(df$GarageYrBlt))] <- 0
for(i in garage.cols){
if (sapply(df[i], is.numeric) == TRUE){
df[,i][which(is.na(df[,i]))] <- 0
}
else{
df[,i][which(is.na(df[,i]))] <- "None"
}
}
df$KitchenQual[which(is.na(df$KitchenQual))] <- Mode(df$KitchenQual)
df[is.na(df$MSZoning),c('MSZoning','MSSubClass')]
table(df$MSZoning, df$MSSubClass)
df$MSZoning[c(2217, 2905)] = 'RL'
df$MSZoning[c(1916, 2251)] = 'RM'
df$LotFrontage[which(is.na(df$LotFrontage))] <- median(df$LotFrontage,na.rm = T)
df$Alley[which(is.na(df$Alley))] <- "None"
#df[(df$MasVnrArea > 0) & (is.na(df$MasVnrType)),c('MasVnrArea','MasVnrType')]
df[2611, 'MasVnrType'] = 'BrkFace'
df$MasVnrType[is.na(df$MasVnrType)] = 'None'
df$MasVnrArea[is.na(df$MasVnrArea)] = 0
for(i in colnames(df[,sapply(df, is.character)])){
if (sum(is.na(df[,i])) < 5){
df[,i][which(is.na(df[,i]))] <- Mode(df[,i])
}
}
for(i in colnames(df[,sapply(df, is.integer)])){
if (sum(is.na(df[,i])) < 5){
df[,i][which(is.na(df[,i]))] <- median(df[,i], na.rm = T)
}
}
for(i in colnames(df[,sapply(df, is.character)])){
df[,i][which(is.na(df[,i]))] <- "None"
}
#colSums(sapply(df, is.na))
sum(is.na(df)) == 1459
train_df <- df[df$isTrain==1,]
test_df <- df[df$isTrain==0,]
train_df$isTrain <- NULL
test_df$isTrain <- NULL
test_df$SalePrice <- NULL
train_df$MSSubClass <- as.factor(train_df$MSSubClass)
test_df$MSSubClass <- as.factor(test_df$MSSubClass)
train_df$OverallQual <- as.factor(train_df$OverallQual)
test_df$OverallQual <- as.factor(test_df$OverallQual)
train_df$OverallCond <- as.factor(train_df$OverallCond)
test_df$OverallCond <- as.factor(test_df$OverallCond)
for(i in colnames(train_df[,sapply(train_df, is.character)])){
train_df[,i] <- as.factor(train_df[,i])
}
for(i in colnames(test_df[,sapply(test_df, is.character)])){
test_df[,i] <- as.factor(test_df[,i])
}
#Check is some there are more levels in some of the categorical factors in the testing compared to the training
for(i in colnames(train_df[,sapply(train_df, is.factor)])){
if (length(levels(train_df[,i])) < length(levels(test_df[,i]))) {
print(i)
print(levels(train_df[,i]))
print(levels(test_df[,i]))
}
}
#df[df$MSSubClass == 150,]
df[df$MSSubClass == 150,"MSSubClass"] <- 120
hist(df$SalePrice)
df$SalePrice <- log(df$SalePrice)
hist(df$SalePrice)
for(i in colnames(df[,sapply(df, is.character)])){
df[,i] <- as.factor(df[,i])
}
df$MSSubClass <- as.factor(df$MSSubClass)
df$OverallQual <- as.factor(df$OverallQual)
df$OverallCond <- as.factor(df$OverallCond)
### THINGS TO CONSIDER:
#df$GarageYrBlt <- as.factor(df$GarageYrBlt) # treat as factor as some of them are '0'
#add years as dummies - POSSIBILITY - but a problem appears, the algorithms cannot treat categorical varibles with more than 55levels
#df$YearBuilt <- as.factor(df$YearBuilt)
#df$YearRemodAdd <- as.factor(df$YearRemodAdd)
#df$YrSold <- as.factor(df$YrSold)
train_df <- df[df$isTrain==1,]
test_df <- df[df$isTrain==0,]
train_df$isTrain <- NULL
test_df$isTrain <- NULL
test_df$SalePrice <- NULL
str(df)
library(boot)
library(leaps)
library(tree) # Normal Tree
library(randomForest) # random Forest
library(xgboost) # boosting trees
library(Matrix) #?
library(methods) #?
library(caret) # for tuning xgboost
X_train<- model.matrix(SalePrice~.-1, data = train_df)
y_train <- train_df$SalePrice
X_test <- model.matrix(~.-1, data=test_df)
cv.ridge <-cv.glmnet(X_train, y_train, nfolds = 10, alpha = 0)
cv.ridge <- cv.glmnet(X_train, y_train, nfolds = 10, alpha = 0)
library(boot)
library(leaps)
library(tree) # Normal Tree
library(randomForest) # random Forest
library(xgboost) # boosting trees
library(Matrix) #?
library(methods) #?
library(caret) # for tuning xgboost
library(boot)
library(leaps)
library(tree) # Normal Tree
library(MASS)
library(randomForest) # random Forest
library(xgboost) # boosting trees
library(Matrix) #?
library(methods) #?
library(caret) # for tuning xgboost
cv.ridge <- cv.glmnet(X_train, y_train, nfolds = 10, alpha = 0)
library(boot)
library(leaps)
library(tree) # Normal Tree
library(glmnet) # Lasso/Ridge
library(randomForest) # random Forest
library(xgboost) # boosting trees
library(Matrix) #?
library(methods) #?
library(caret) # for tuning xgboost
cv.ridge <- cv.glmnet(X_train, y_train, nfolds = 10, alpha = 0)
plot(cv.ridge)
#here is an example why should we apply the one standard deviation error rule!
penalty_min <- cv.ridge$lambda.min #optimal lambda
penalty_1se <- cv.ridge$lambda.1se # 1 Standard Error Apart
fit.ridge_min <-glmnet(X_train, y_train, alpha = 1, lambda = penalty_min) #estimate the model with min lambda
fit.ridge_1se <-glmnet(X_train, y_train, alpha = 1, lambda = penalty_1se) #estimate the model with 1se apart
prediction_ridge_min_log <- predict(fit.ridge_min, X_test)
prediction_ridge_1se_log <- predict(fit.ridge_1se, X_test)
prediction_ridge_min <- exp(prediction_ridge_min_log)
prediction_ridge_1se <- exp(prediction_ridge_1se_log)
prediction_ridge_min <-cbind(Id = rownames(test_df), SalesPrice = prediction_ridge_min)
prediction_ridge_1se <- cbind(Id = rownames(test_df), SalesPrice = prediction_ridge_1se)
write.table(prediction_ridge_min, file="prediction_ridge_min.csv",col.names = c("Id", "SalePrice"), sep =',', row.names = FALSE)
write.table(prediction_ridge_1se, file="prediction_ridge_1se.csv",col.names = c("Id", "SalePrice"), sep =',', row.names = FALSE)
## Regression Tree
tree.SalePrice <-tree(SalePrice~., data = train_df)
summary(tree.SalePrice)
plot(tree.SalePrice)
text(tree.SalePrice, pretty=1)
## Regression Tree
tree.SalePrice <-tree(SalePrice~., data = train_df)
#summary(tree.SalePrice)
plot(tree.SalePrice)
text(tree.SalePrice, pretty=1)
mean(train_df$SalePrice)
train_df$Classifier <- ifelse(train_df$SalePrice <= 12.024,"Low","High")
train_df$Classifier <- as.factor(train_df$Classifier)
train_df$Classifier <- factor(train_df$Classifier, levels = c("Low", "High"))
attach(train_df)
set.seed(1)
training<- sample(1:nrow(train_df), 1460*0.5)
test.train_df <- train_df[-training,]
train.train_df <- train_df[training,]
Classifier.test <- test.train_df$Classifier
x.train.classifier <- model.matrix(Classifier~.-SalePrice, data=train.train_df)
y.train.classifier <- train.train_df$Classifier
x.test.classifier <- model.matrix(Classifier~.-SalePrice, data=test.train_df)
fit.ridge <- glmnet(x.train.classifier,y.train.classifier,alpha=0, family="binomial")
plot(fit.ridge, xvar='lambda', lanel=TRUE)
plot(fit.ridge, xvar='dev', lanel=TRUE)
cv.ridge <- cv.glmnet(x.train.classifier,y.train.classifier,alpha=0,family="binomial")
plot(cv.ridge)
fit.ridge <- glmnet(x.train.classifier,y.train.classifier,alpha=0, family="binomial")
plot(fit.ridge, xvar='lambda', label=TRUE)
plot(fit.ridge, xvar='dev', label=TRUE)
cv.ridge <- cv.glmnet(x.train.classifier,y.train.classifier,alpha=0,family="binomial")
plot(cv.ridge)
ridge.min.lambda=cv.ridge$lambda.min
fit.ridge.min <- glmnet(x.train.classifier,y.train.classifier,alpha=0, family="binomial", lambda=ridge.min.lambda)
prediction.ridge.min.log <- predict(fit.ridge.min, x.test.classifier)
prediction.ridge.min.log.classifier <- (ifelse(prediction.ridge.min.log >0.5,1,0))
table(prediction.ridge.min.log.classifier, Classifier.test)
ridge.1se.lambda=cv.ridge$lambda.1se
fit.ridge.1se <- glmnet(x.train.classifier,y.train.classifier,alpha=0, family="binomial", lambda=ridge.1se.lambda)
prediction.ridge.1se.log <- predict(fit.ridge.1se, x.test.classifier)
prediction.ridge.1se.log.classifier <- ifelse(prediction.ridge.1se.log >0.5,1,0)
table(prediction.ridge.1se.log.classifier, Classifier.test)
fit.lasso <- glmnet(x.train.classifier,y.train.classifier, family="binomial")
plot(fit.lasso, xvar='lambda', lanel=TRUE)
plot(fit.lasso, xvar='dev', lanel=TRUE)
cv.lasso <- cv.glmnet(x.train.classifier,y.train.classifier ,family="binomial")
plot(cv.lasso)
lasso.min.lambda=cv.lasso$lambda.min
fit.lasso.min <- glmnet(x.train.classifier,y.train.classifier, family="binomial", lambda=lasso.min.lambda)
prediction.lasso.min.log <- predict(fit.lasso.min, x.test.classifier)
prediction.lasso.min.log.classifier <- (ifelse(prediction.lasso.min.log >0.5,1,0))
table(prediction.lasso.min.log.classifier, Classifier.test)
lasso.1se.lambda=cv.lasso$lambda.1se
fit.lasso.1se <- glmnet(x.train.classifier,y.train.classifier,alpha=0, family="binomial", lambda=lasso.1se.lambda)
prediction.lasso.1se.log <- predict(fit.lasso.1se, x.test.classifier)
prediction.lasso.1se.log.classifier <- ifelse(prediction.lasso.1se.log >0.5,1,0)
table(prediction.lasso.1se.log.classifier, Classifier.test)
tree.train_df <- tree(Classifier~.-SalePrice, train.train_df)
tree.pred <- predict(tree.train_df, test.train_df, type="class")
length(tree.pred)
length(Classifier.test)
table(tree.pred, Classifier.test)
mean(tree.pred!=Classifier.test)
tree.pred <- predict(tree.train_df, test.train_df, type="class")
length(tree.pred)
length(Classifier.test)
table(tree.pred, Classifier.test)
mean(tree.pred!=Classifier.test)
cv.train_df <- cv.tree(tree.train_df, FUN= prune.misclass)
par(mfrow=c(1,2))
plot(cv.train_df$size, cv.train_df$dev, type="b")
plot(cv.train_df$k, cv.train_df$dev, type="b")
par(mfrow=c(1,1))
prune.train_df <-prune.misclass(tree.train_df, best=9)
plot(prune.train_df)
text(prune.train_df, pretty=0)
tree.pred <-predict(prune.train_df, test.train_df, type="class")
table(tree.pred,Classifier.test)
mean(tree.pred!=Classifier.test)
bag.train_df <- randomForest(Classifier~. -SalePrice, data=train.train_df, mtry=79, importance=TRUE)
bag.train_df
bag.classifier <- predict(bag.train_df, newdata = test.train_df)
table(predict=bag.classifier, truth=Classifier.test)
mean(bag.classifier!=Classifier.test)
rf.train_df <- randomForest(Classifier~. -SalePrice, data=train.train_df, mtry=26, importance=TRUE)
rf.train_df
rf.classifier <- predict(rf.train_df, newdata = test.train_df)
table(predict=rf.classifier, truth=Classifier.test)
mean(rf.classifier!=Classifier.test)
svmfit <-svm(Classifier~.-SalePrice, data=train.train_df, kernel="linear", cost=10, scale=FALSE)
setwd("C:/Users/Michal/Documents/01- Master Degree/GitHub/ST443-Project-group9/Housing price data")
getwd()
setwd("C:/Users/Michal/Documents/01- Master Degree/ST447 Data Analysis and Statistical Methods/Group project")
UCBA <- read.csv("UCBA.csv",header=TRUE,sep=";") #Load UCBA data set
colnames(UCBA)[1] <- "Department"
#Transform categorical variables into integer to input in sparsebn
UCBA$Department<-as.integer(as.factor(UCBA$Department))
UCBA$Gender<-as.integer(as.factor(UCBA$Gender))
UCBA$Decision<-as.integer(as.factor(UCBA$Decision))
View(UCBA)
View(UCBA)
UCBA <- read.csv("UCBA.csv",header=TRUE,sep=";") #Load UCBA data set
View(UCBA)
View(UCBA)
solution <- select(dags, edges=3)
UCBA <- read.csv("UCBA.csv",header=TRUE,sep=";") #Load UCBA data set
colnames(UCBA)[1] <- "Department"
UCBA$Department<-as.integer(as.factor(UCBA$Department))
UCBA$Gender<-as.integer(as.factor(UCBA$Gender))
UCBA$Decision<-as.integer(as.factor(UCBA$Decision))
data <- sparsebnData(UCBA, type = "discrete")
UCBA <- read.csv("UCBA.csv",header=TRUE,sep=";") #Load UCBA data set
colnames(UCBA)[1] <- "Department"
UCBA$Department<-as.integer(as.factor(UCBA$Department))
UCBA$Gender<-as.integer(as.factor(UCBA$Gender))
UCBA$Decision<-as.integer(as.factor(UCBA$Decision))
data <- sparsebnData(UCBA, type = "discrete")
library(ggplot2)
library(sparsebn)
data <- sparsebnData(UCBA, type = "discrete")
dags <- estimate.dag(data)
data <- sparsebnData(UCBA, type = "discrete")
dags <- estimate.dag(data)
solution <- select(dags, edges=3)
solution
solution <- select(dags, edges=3)
solution
solution <- select(dags, edges=3)
solution$edges
solution <- select(dags, edges=3)
solution$nedge
solution <- select(dags, edges=3)
solution
solution <- select(dags, edges=3)
solution$nodes
solution <- select(dags, edges=3)
solution$pp
solution <- select(dags, edges=3)
solution$nn
solution <- select(dags, edges=3)
solution$time
solution <- select(dags, edges=3)
solution$edges
solution <- select(dags, edges=3)
solution
plot(solution,
layout = igraph::layout_(to_igraph(solution$edges), igraph::in_circle()),
vertex.label = solution$nodes,
vertex.label.dist = -2,
vertex.size = 7,
vertex.label.color = gray(0),
vertex.color = gray(0.9),
edge.color = gray(0),
edge.arrow.size = 0.7)
