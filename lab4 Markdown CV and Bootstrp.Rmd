---
title: "Lab4 Markdown"
output: html_document
---
```{r}
library(ISLR)
plot(mpg~horsepower,data=Auto)
dim(Auto)
attach(Auto)
fix(Auto)
```

```{r}
##############################################################################################################################
# A validation set approach
# Randomly split the full data into half training and half testing

set.seed(1) ## generate the same number of random variables
train <-sample(392,196)
```

```{r}
# linear model: regree mpg on horsepower using training data set
lm.fit <-lm(mpg~horsepower,data=Auto,subset=train)
summary(lm.fit)
```

#####calculating the MSE of Training data
```{r}
# -train index below selects only the observations that are not in the training set
# MSE OF 196 observations in the validation set
mean((mpg-predict(lm.fit,Auto))[-train]^2)
```
```{r}
# we use poly() function to estimate the test erro for the polynomial regressions
lm.fit2 <-lm(mpg~ poly(horsepower,2),data=Auto,subset=train)
mean((mpg-predict(lm.fit2,Auto))[-train]^2)

lm.fit3 <-lm(mpg~ poly(horsepower,3),data=Auto,subset=train)
mean((mpg-predict(lm.fit3,Auto))[-train]^2)
```
```{r}
library(boot)
# Leave-one-out cv (LOOCV)
glm.fit <-glm(mpg~horsepower,data=Auto)
coef(glm.fit)
# cv.glm can be used to perform cv, if we use glm() to fit a model without passing in the family argument, then it performs linear regression like lm() function
?cv.glm
cv.glm(Auto, glm.fit)$delta ## Very slow (does not use formula (5.2) on page 180)

# write a simple function to use formua (5.2)
loocv <-function(fit){
  h <-lm.influence(fit)$h
  mean((residuals(fit)/(1-h))^2)
}
loocv(glm.fit)
```
```{r}
# Plot the cv errors vs degree of the polynomial
cv.error1 <-rep(0,5)
cv.error2 <-rep(0,5)
```
######testing the CV-error for different polynomial degrees
```{r}
degree <-1:5
for(i in degree){
  glm.fit <-glm(mpg~ poly(horsepower,i), data=Auto)
  ## CV errors using formula (5.2)
  cv.error1[i] <- loocv(glm.fit)
  ## CV errors using naive LOOCV
  cv.error2[i] <- cv.glm(Auto, glm.fit)$delta[1]
}
```

```{r}
cv.error1
cv.error2
plot(degree, cv.error1, type="b")
lines(degree, cv.error2, col="red")
```
```{r}
################################################################################################################################
## 10 fold cross validation, you can try 5 fold CV by setting K=5  in cv.glm function
cv.error10 <-rep(0,5)
for(i in degree){
  glm.fit <-glm(mpg~poly(horsepower,i), data=Auto)
  # Note the value of K is the number of groups which the data should be split to estimate the CV error, by default K=n, i.e. LOOCV !!!!!!!!!!!!!!!!!!!!!!!
  cv.error10[i] <-cv.glm(Auto, glm.fit, K=10)$delta[1] 
}
cv.error10
plot(degree, cv.error1, type="b")
lines(degree, cv.error2, col="red")
lines(degree, cv.error10, type="b", col="blue")  

```
####BOOTSTRAPING
```{r}
fix(Portfolio)
```

```{r}
################################################################################################################################
# Bootstrap
# We use Portfolio data set in the ISLR package
# alph.fn() function takes as input the (X,Y) data as well as a vector indicating which observations should be used to estimate alpha
alpha.fn <-function(data,index){
  X <-data$X[index]
  Y <-data$Y[index]
  (var(Y) -cov(X,Y))/(var(X)+ var(Y) - 2*cov(X,Y))
}

set.seed(1)
alpha.fn(Portfolio, 1:100)
# randomly select 100 observations from 1 to 100 with replacement, i.e. constract a new bootstrap data and compute the corresponding alpha
alpha.fn(Portfolio, sample(100,100, replace=T))

# Produce R=1000 bootstrap estimates for alpha
boot(Portfolio, alpha.fn, R=1000)

```
##### Estimating the accuracy of a linear regression model
```{r}
################################################################################################################################

boot.fn <-function(data,index){
  coef(lm(mpg~horsepower, data=data,subset=index))
}
```

```{r}
# This returns the intercept and slope estimates for the linear regression model
boot.fn(Auto, 1:392)
set.seed(1)
boot.fn(Auto, sample(392,392, replace=T))

# Now we use boot() to compute the standard errors of 1000 bootstrap estimates for the intercept and slope
boot(Auto, boot.fn, R=1000)


```
```{r}
# Compare with standard formula results for the regression coefficients in a linear model
summary(lm(mpg~horsepower, data=Auto))$coef
# What can you conclude from the different results?
```
```{r}
# Redo everything for polynomial regression with degree=2
boot.fn <-function(data, index)
  coefficients(lm(mpg~ horsepower + I(horsepower^2), data=data, subset=index))
set.seed(1)
# Bootstrap with 1000 replications
boot(Auto, boot.fn, 1000)
summary(lm(mpg~ horsepower + I(horsepower^2), data= Auto))$coef
```


